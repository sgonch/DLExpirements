{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gym import utils, spaces\n",
    "from gym.envs.vmware_env.esxtest_env import ESXTestEnv\n",
    "from gym.envs.vmware_env.vmware_env import VMwareEnv\n",
    "\n",
    "\n",
    "from gym.utils import seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QLearn:\n",
    "    def __init__(self, actions, epsilon, alpha, gamma):\n",
    "        self.q = {}\n",
    "        self.epsilon = epsilon  # exploration constant\n",
    "        self.alpha = alpha      # discount constant\n",
    "        self.gamma = gamma      # discount factor\n",
    "        self.actions = actions\n",
    "\n",
    "    def getQ(self, state, action):\n",
    "        return self.q.get((state, action), 0.0)\n",
    "\n",
    "    def learnQ(self, state, action, reward, value):\n",
    "        '''\n",
    "        Q-learning:\n",
    "            Q(s, a) += alpha * (reward(s,a) + max(Q(s') - Q(s,a))            \n",
    "        '''\n",
    "        oldv = self.q.get((state, action), None)\n",
    "        if oldv is None:\n",
    "            self.q[(state, action)] = reward\n",
    "        else:\n",
    "            self.q[(state, action)] = oldv + self.alpha * (value - oldv)\n",
    "\n",
    "    def chooseAction(self, state, return_q=False):\n",
    "        q = [self.getQ(state, a) for a in self.actions]\n",
    "        maxQ = max(q)\n",
    "\n",
    "        if random.random() < self.epsilon:\n",
    "            minQ = min(q); mag = max(abs(minQ), abs(maxQ))\n",
    "            # add random values to all the actions, recalculate maxQ\n",
    "            q = [q[i] + random.random() * mag - .5 * mag for i in range(len(self.actions))] \n",
    "            maxQ = max(q)\n",
    "\n",
    "        count = q.count(maxQ)\n",
    "        # In case there're several state-action max values \n",
    "        # we select a random one among them\n",
    "        if count > 1:\n",
    "            best = [i for i in range(len(self.actions)) if q[i] == maxQ]\n",
    "            i = random.choice(best)\n",
    "        else:\n",
    "            i = q.index(maxQ)\n",
    "\n",
    "        action = self.actions[i]        \n",
    "        if return_q: # if they want it, give it!\n",
    "            return action, q\n",
    "        return action\n",
    "\n",
    "    def learn(self, state1, action1, reward, state2):\n",
    "        maxqnew = max([self.getQ(state2, a) for a in self.actions])\n",
    "        self.learnQ(state1, action1, reward, reward + self.gamma*maxqnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from gym.envs.registration import register\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Virtualization\n",
    "# ----------------------------------------\n",
    "\n",
    "\n",
    "register(\n",
    "    id='VirtESX-v0',\n",
    "    entry_point='gym.envs.vmware_env.esxtest_env:ESXTestEnv',\n",
    "    # More arguments here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-08 00:04:57,365] Making new env: VirtESX-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VMWare Env launched!\n",
      "starting...\n",
      "resetting...\n",
      "Resetting 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-08 00:05:08,915] Disabling video recorder because <ESXTestEnv instance> neither supports video mode \"rgb_array\" nor \"ansi\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting 2...\n",
      "Resetting 3...\n",
      "reset...\n",
      "Step: 1  .\n",
      "Step: 2  .\n",
      "Step: 3  .\n",
      "Step: 4  .\n",
      "Step: 5  .\n",
      "Step: 6  .\n",
      "Step: 7  .\n",
      "Step: 8  .\n",
      "Step: 9  .\n",
      "Step: 10  .\n",
      "Step: 11  .\n",
      "Step: 12  .\n",
      "Step: 13  .\n",
      "Step: 14  .\n",
      "Step: 15  .\n",
      "Step: 16  .\n",
      "Step: 17  .\n",
      "Step: 18  .\n",
      "Step: 19  .\n",
      "Step: 20  .\n",
      "Step: 21  .\n",
      "Step: 22  .\n",
      "Step: 23  .\n",
      "Step: 24  .\n",
      "Step: 25  .\n",
      "Step: 26  .\n",
      "Step: 27  .\n",
      "Step: 28  .\n",
      "Step: 29  .\n",
      "Step: 30  .\n",
      "Step: 31  .\n",
      "Step: 32  .\n",
      "Step: 33  .\n",
      "Step: 34  .\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-bf6785f4e171>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[1;31m# Execute the action and get feedback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m             \u001b[0mcumulated_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\local\\Anaconda3-4.1.1-Windows-x86_64\\envs\\cntk-py35\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    120\u001b[0m         \"\"\"\n\u001b[0;32m    121\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_before_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_after_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\local\\Anaconda3-4.1.1-Windows-x86_64\\envs\\cntk-py35\\lib\\site-packages\\gym\\envs\\vmware_env\\esxtest_env.py\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPCLICommand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvmid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0maction\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#+CPU\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPCLICommand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvmid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0maction\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#-CPU\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPCLICommand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvmid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\local\\Anaconda3-4.1.1-Windows-x86_64\\envs\\cntk-py35\\lib\\site-packages\\gym\\envs\\vmware_env\\esxtest_env.py\u001b[0m in \u001b[0;36mPCLICommand\u001b[1;34m(self, command, vmid)\u001b[0m\n\u001b[0;32m     25\u001b[0m         output = subprocess.check_output(\n\u001b[0;32m     26\u001b[0m               \u001b[1;33m[\u001b[0m\u001b[1;34m\"powershell.exe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"C:\\CNTK\\cntk\\Tutorials\\DGTest\\ConnectESX.ps1 -vmid {0} -action {1}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvmid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m               shell=True)\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;31m#print(output)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m#print(\"C:\\CNTK\\cntk\\Tutorials\\DGTest\\ConnectESX.ps1 -vmid {1} -action {2}\"%(vmid,command))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\local\\Anaconda3-4.1.1-Windows-x86_64\\envs\\cntk-py35\\lib\\subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[1;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[1;32m--> 626\u001b[1;33m                **kwargs).stdout\n\u001b[0m\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\local\\Anaconda3-4.1.1-Windows-x86_64\\envs\\cntk-py35\\lib\\subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(input, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    693\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 695\u001b[1;33m             \u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    696\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTimeoutExpired\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    697\u001b[0m             \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\local\\Anaconda3-4.1.1-Windows-x86_64\\envs\\cntk-py35\\lib\\subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[1;34m(self, input, timeout)\u001b[0m\n\u001b[0;32m   1057\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stdin_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1058\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1059\u001b[1;33m                 \u001b[0mstdout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1060\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1061\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class LivePlot(object):\n",
    "    def __init__(self, outdir, data_key='episode_rewards', line_color='blue'):\n",
    "        \"\"\"\n",
    "        Liveplot renders a graph of either episode_rewards or episode_lengths\n",
    "        Args:\n",
    "            outdir (outdir): Monitor output file location used to populate the graph\n",
    "            data_key (Optional[str]): The key in the json to graph (episode_rewards or episode_lengths).\n",
    "            line_color (Optional[dict]): Color of the plot.\n",
    "        \"\"\"\n",
    "        self.outdir = outdir\n",
    "        self._last_data = None\n",
    "        self.data_key = data_key\n",
    "        self.line_color = line_color\n",
    "\n",
    "        #styling options\n",
    "        matplotlib.rcParams['toolbar'] = 'None'\n",
    "        plt.style.use('ggplot')\n",
    "        plt.xlabel(\"\")\n",
    "        plt.ylabel(data_key)\n",
    "        fig = plt.gcf().canvas.set_window_title('simulation_graph')\n",
    "\n",
    "    def plot(self):\n",
    "        results = gym.monitoring.monitor.load_results(self.outdir)\n",
    "        data =  results[self.data_key]\n",
    "\n",
    "        #only update plot if data is different (plot calls are expensive)\n",
    "        if data !=  self._last_data:\n",
    "            self._last_data = data\n",
    "            plt.plot(data, color=self.line_color)\n",
    "\n",
    "            # pause so matplotlib will display\n",
    "            # may want to figure out matplotlib animation or use a different library in the future\n",
    "            plt.pause(0.000001)\n",
    "\n",
    "def render():\n",
    "    render_skip = 0 #Skip first X episodes.\n",
    "    render_interval = 50 #Show render Every Y episodes.\n",
    "    render_episodes = 10 #Show Z episodes every rendering.\n",
    "\n",
    "    if (x%render_interval == 0) and (x != 0) and (x > render_skip):\n",
    "        env.render()\n",
    "    elif ((x-render_episodes)%render_interval == 0) and (x != 0) and (x > render_skip) and (render_episodes < x):\n",
    "        env.render(close=True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    env = gym.make('VirtESX-v0')\n",
    "\n",
    "\n",
    "    outdir = '/tmp/vmware_gym_experiments'\n",
    "    env.monitor.start(outdir, force=True)\n",
    "\n",
    "    #plotter = LivePlot(outdir)\n",
    "\n",
    "    last_time_steps = np.ndarray(0)\n",
    "\n",
    "    qlearn = QLearn(actions=range(env.action_space.n),\n",
    "                    alpha=0.1, gamma=0.8, epsilon=0.9)\n",
    "\n",
    "    initial_epsilon = qlearn.epsilon\n",
    "\n",
    "    epsilon_discount = 0.999 # 1098 eps to reach 0.1\n",
    "\n",
    "    start_time = time.time()\n",
    "    total_episodes = 10000\n",
    "    highest_reward = 0\n",
    "\n",
    "    print(\"starting...\")\n",
    "    \n",
    "    for x in range(total_episodes):\n",
    "        done = False\n",
    "\n",
    "        cumulated_reward = 0 #Should going forward give more reward then L/R ?\n",
    "        print(\"resetting...\")\n",
    "        observation = env.reset()\n",
    "        print(\"reset...\")\n",
    "        if qlearn.epsilon > 0.05:\n",
    "            qlearn.epsilon *= epsilon_discount\n",
    "\n",
    "        #render() #defined above, not env.render()\n",
    "\n",
    "        state = ''.join(map(str, observation))\n",
    "\n",
    "        for i in range(500):\n",
    "            print (\"Step: \"+str(i+1)+\"  .\")\n",
    "            # Pick an action based on the current state\n",
    "            action = qlearn.chooseAction(state)\n",
    "\n",
    "            # Execute the action and get feedback\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            cumulated_reward += reward\n",
    "\n",
    "            if highest_reward < cumulated_reward:\n",
    "                highest_reward = cumulated_reward\n",
    "\n",
    "            nextState = ''.join(map(str, observation))\n",
    "\n",
    "            qlearn.learn(state, action, reward, nextState)\n",
    "\n",
    "            env.monitor.flush(force=True)\n",
    "\n",
    "            if not(done):\n",
    "                state = nextState\n",
    "            else:\n",
    "                last_time_steps = np.append(last_time_steps, [int(i + 1)])\n",
    "                break \n",
    "\n",
    "        m, s = divmod(int(time.time() - start_time), 60)\n",
    "        h, m = divmod(m, 60)\n",
    "        print (\"EP: \"+str(x+1)+\" - [alpha: \"+str(round(qlearn.alpha,2))+\" - gamma: \"+str(round(qlearn.gamma,2))+\" - epsilon: \"+str(round(qlearn.epsilon,2))+\"] - Reward: \"+str(cumulated_reward)+\"     Time: %d:%02d:%02d\" % (h, m, s))\n",
    "\n",
    "    #Github table content\n",
    "    print (\"\\n|\"+str(total_episodes)+\"|\"+str(qlearn.alpha)+\"|\"+str(qlearn.gamma)+\"|\"+str(initial_epsilon)+\"*\"+str(epsilon_discount)+\"|\"+str(highest_reward)+\"| PICTURE |\")\n",
    "\n",
    "    l = last_time_steps.tolist()\n",
    "    l.sort()\n",
    "\n",
    "    #print(\"Parameters: a=\"+str)\n",
    "    print(\"Overall score: {:0.2f}\".format(last_time_steps.mean()))\n",
    "    print(\"Best 100 score: {:0.2f}\".format(reduce(lambda x, y: x + y, l[-100:]) / len(l[-100:])))\n",
    "\n",
    "    #env.monitor.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
